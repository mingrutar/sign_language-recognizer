{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "                    SelectorBIC,         SelectorDIC,        SelectorCV\n",
    "features_ground,  0.5617977528089888   0.5786516853932584   0.5617977528089888\n",
    "                   78 out of 178        75 out of 178        78 out of 178\n",
    "features_polar    0.5280898876404494   0.5449438202247191   0.5337078651685393\n",
    "                  84 out of 178         81 out of 178         83 out of 178\n",
    "                  \n",
    "model_selector=SelectorBIC_2, where param = num_state * num_state + 2 * num_state  - 1\n",
    "\n",
    "feature=features_podel\n",
    "**** WER = 0.5112359550561798\n",
    "Total correct: 87 out of 178\n",
    "\n",
    "feature=features_podel\n",
    "**** WER = 0.5112359550561798\n",
    "Total correct: 87 out of 178\n",
    "----------------------------------------------------------------------------------------------\n",
    "customed features       BIC                                    DIC\n",
    "features_norm_poldel   0.5842696629213483           0.5898876404494382\n",
    "                        74 out of 178                73 /178\n",
    "features_podel,        0.5                          0.5337078651685393\n",
    "                       89 out of 178                 83/178\n",
    "features_norm_pol      0.5842696629213483           0.5674157303370787\n",
    "                       74 out of 178                  77 out of 178\n",
    "                       </code>\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x  left-y  right-x  right-y  nose-x  nose-y  speaker\n",
       "video frame                                                           \n",
       "98    0         149     181      170      175     161      62  woman-1\n",
       "      1         149     181      170      175     161      62  woman-1\n",
       "      2         149     181      170      175     161      62  woman-1\n",
       "      3         149     181      170      175     161      62  woman-1\n",
       "      4         149     181      170      175     161      62  woman-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from asl_data import AslDb\n",
    "asl = AslDb() # initializes the database\n",
    "asl.df.head() # displays the first five rows of the asl database, indexed by video and frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['J', 'O'], ['O', 'H'], ['H', 'N'], ['N', ' '], [' ', 'F'], ['F', 'I'], ['I', 'S'], ['S', 'H'], ['H', ' '], [' ', 'W'], ['W', 'O'], ['O', 'N'], ['N', 'T'], ['T', ' '], [' ', 'E'], ['E', 'A'], ['A', 'T'], ['T', ' '], [' ', 'B'], ['B', 'U'], ['U', 'T'], ['T', ' '], [' ', 'C'], ['C', 'A'], ['A', 'N'], ['N', ' '], [' ', 'E'], ['E', 'A'], ['A', 'T'], ['T', ' '], [' ', 'C'], ['C', 'H'], ['H', 'I'], ['I', 'C'], ['C', 'K'], ['K', 'E'], ['E', 'N']]\n",
      "next ngram\n",
      "['JO', 'OH', 'HN', 'N ', ' F', 'FI', 'IS', 'SH', 'H ', ' W', 'WO', 'ON', 'NT', 'T ', ' E', 'EA', 'AT', 'T ', ' B', 'BU', 'UT', 'T ', ' C', 'CA', 'AN', 'N ', ' E', 'EA', 'AT', 'T ', ' C', 'CH', 'HI', 'IC', 'CK', 'KE', 'EN']\n",
      "next sentence\n",
      "[['M', 'A'], ['A', 'R'], ['R', 'Y'], ['Y', ' '], [' ', 'V'], ['V', 'E'], ['E', 'G'], ['G', 'E'], ['E', 'T'], ['T', 'A'], ['A', 'B'], ['B', 'L'], ['L', 'E'], ['E', ' '], [' ', 'K'], ['K', 'N'], ['N', 'O'], ['O', 'W'], ['W', ' '], [' ', 'I'], ['I', 'X'], ['X', ' '], [' ', 'L'], ['L', 'I'], ['I', 'K'], ['K', 'E'], ['E', ' '], [' ', 'C'], ['C', 'O'], ['O', 'R'], ['R', 'N'], ['N', '1']]\n",
      "next ngram\n",
      "['MA', 'AR', 'RY', 'Y ', ' V', 'VE', 'EG', 'GE', 'ET', 'TA', 'AB', 'BL', 'LE', 'E ', ' K', 'KN', 'NO', 'OW', 'W ', ' I', 'IX', 'X ', ' L', 'LI', 'IK', 'KE', 'E ', ' C', 'CO', 'OR', 'RN', 'N1']\n",
      "next sentence\n"
     ]
    }
   ],
   "source": [
    "def unigrams(text):\n",
    "    uni = []\n",
    "    for token in text:\n",
    "        uni.append([token])\n",
    "    return uni\n",
    "\n",
    "def bigrams(text):\n",
    "    bi = []\n",
    "    token_address = 0\n",
    "    for token in text[:len(text) - 1]:\n",
    "        bi.append([token, text[token_address + 1]])\n",
    "        token_address += 1\n",
    "    return bi\n",
    "\n",
    "def trigrams(text):\n",
    "    tri = []\n",
    "    token_address = 0\n",
    "    for token in text[:len(text) - 2]:\n",
    "        tri.append([token, text[token_address + 1], text[token_address + 2]])\n",
    "        token_address += 1\n",
    "    return tri\n",
    "\n",
    "def ngram(text, grams):\n",
    "    model=[]\n",
    "    count=0\n",
    "    for token in text[:len(text)-grams+1]:\n",
    "       model.append(text[count:count+grams])\n",
    "       count=count+1\n",
    "    return model\n",
    "\n",
    "sentences = [\"JOHN FISH WONT EAT BUT CAN EAT CHICKEN\", \"MARY VEGETABLE KNOW IX LIKE CORN1\"]\n",
    "for t in sentences:\n",
    "    print(bigrams(t))\n",
    "    print('next ngram')\n",
    "    print(ngram(t, 2))\n",
    "    print('next sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\linna/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py35\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py35\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\linna\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3c2fd5008784>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m sentence = \"\"\"At eight o'clock on Thursday morning\n\u001b[0;32m      4\u001b[0m ... Arthur didn't feel very good.\"\"\"\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmy_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_bigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    132\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\linna/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py35\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py35\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\linna\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#sentences = [\"JOHN FISH WONT EAT BUT CAN EAT CHICKEN\", \"MARY VEGETABLE KNOW IX LIKE CORN1\"]\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "my_bigrams = nltk.bigrams(words)\n",
    "print(my_bigrams)\n",
    "my_trigrams = nltk.trigrams(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:9: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 113, -12, 119]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add df columns for 'grnd-rx', 'grnd-ly', 'grnd-lx' representing differences between hand and nose locations\n",
    "asl.df['grnd-ry'] = asl.df['right-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-rx'] = asl.df['right-x'] - asl.df['nose-x']\n",
    "asl.df['grnd-ly'] = asl.df['left-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-lx'] = asl.df['left-x'] - asl.df['nose-x']\n",
    "\n",
    "features_ground = ['grnd-rx','grnd-ry','grnd-lx','grnd-ly']\n",
    " #show a single set of features for a given (video, frame) tuple\n",
    "[asl.df.ix[98,1][v] for v in features_ground]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[113.35784048754634,\n",
       " 0.079478244608206572,\n",
       " 119.60351165413162,\n",
       " -0.10050059905462982]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']\n",
    "asl.df['polar-rr'] = np.sqrt((asl.df['right-x']- asl.df['nose-x'])**2 + (asl.df['right-y']-asl.df['nose-y'])**2)\n",
    "asl.df['polar-rtheta'] = np.arctan2(asl.df['right-x']- asl.df['nose-x'],asl.df['right-y'] - asl.df['nose-y'])\n",
    "asl.df['polar-lr'] = np.sqrt((asl.df['left-x']-asl.df['nose-x'])**2 + (asl.df['left-y']-asl.df['nose-y'])**2)\n",
    "asl.df['polar-ltheta'] = np.arctan2(asl.df['left-x']- asl.df['nose-x'], asl.df['left-y'] - asl.df['nose-y'])\n",
    "[asl.df.ix[98,1][v] for v in features_polar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29.698484809834994,\n",
       " -0.78539816339744828,\n",
       " 114.83332419142495,\n",
       " -0.12983714941684962]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add features of your own design, which may be a combination of the above or something else\n",
    "# Name these whatever you would like\n",
    "features_podel = ['podel-rr', 'podel-rtheta', 'podel-lr', 'podel-ltheta']\n",
    "asl.df['podel-rr'] = asl.df['polar-rr'] + asl.df['polar-rr'].diff().fillna(0)\n",
    "asl.df['podel-rtheta'] = asl.df['polar-rtheta'] + asl.df['polar-rtheta'].diff().fillna(0)\n",
    "asl.df['podel-lr'] = asl.df['polar-lr'] + asl.df['polar-lr'].diff().fillna(0)\n",
    "asl.df['podel-ltheta'] = asl.df['polar-ltheta'] + asl.df['polar-ltheta'].diff().fillna(0)\n",
    "[asl.df.ix[98,36][v] for v in features_podel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asl.df['polar-rr'].max(), asl.df['polar-rr'].min()\n",
    "asl.df['polar-lr'].diff().max(), asl.df['polar-ltheta'].diff().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29.698484809834994,\n",
       " -0.78539816339744828,\n",
       " 108.75126904475623,\n",
       " -0.016498269416752684]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_npodel = ['npodel-rr', 'npodel-rtheta', 'npodel-lr', 'npodel-ltheta']\n",
    "asl.df['npodel-rr'] = asl.df['polar-rr']/asl.df['polar-rr'].max() \\\n",
    "+ asl.df['polar-rr'].diff().fillna(0)/asl.df['polar-rr'].diff().max()\n",
    "asl.df['npodel-rtheta'] = asl.df['polar-rtheta']/asl.df['polar-rtheta'].max() \\\n",
    "+ asl.df['polar-rtheta'].diff().fillna(0) /asl.df['polar-rtheta'].diff().max()\n",
    "asl.df['npodel-lr'] = asl.df['polar-lr']/asl.df['polar-lr'].max() + \\\n",
    "asl.df['polar-lr'].diff().fillna(0) /asl.df['polar-lr'].diff().max()\n",
    "asl.df['npodel-ltheta'] = asl.df['polar-ltheta']/asl.df['polar-ltheta'].max() \\\n",
    "+ asl.df['polar-ltheta'].diff().fillna(0)/asl.df['polar-ltheta'].diff().max()\n",
    "[asl.df.ix[98,35][v] for v in features_podel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from asl_utils import show_errors\n",
    "import warnings\n",
    "from asl_data import SinglesData\n",
    "\n",
    "def recognize(models: dict, test_set: SinglesData):\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    probabilities = []\n",
    "    wid_list = sorted(test_set.get_all_sequences().keys())\n",
    "    for wid in wid_list:\n",
    "        X, lengths = test_set.get_item_Xlengths(wid)\n",
    "        # print('wid={}, Xlen={}'.format(wid, test_Xlength_dict[wid]))\n",
    "        all_score = {}\n",
    "        for word, model in models.items():\n",
    "            if model:\n",
    "                try:\n",
    "                    with np.errstate(divide='ignore'):\n",
    "                        logL = model.score(X, lengths)\n",
    "                    all_score[word] = logL\n",
    "                    # print('!! wid={}, word={}, logL={}'.format(wid, word, logL))\n",
    "                except:\n",
    "                    pass\n",
    "                    # print('missed wid={}, word={}'.format(wid, word))\n",
    "        probabilities.append(all_score)\n",
    "    guesses = [sorted(sd.items(),key=lambda x: x[1], reverse=True)[0][0] for sd in probabilities]\n",
    "    return probabilities, guesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload for automatically reloading changes made in my_model_selectors and my_recognizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from my_model_selectors import SelectorBIC\n",
    "def train_all_words(features, model_selector):\n",
    "    training = asl.build_training(features)  # Experiment here with different feature sets defined in part 1\n",
    "    sequences = training.get_all_sequences()\n",
    "    Xlengths = training.get_all_Xlengths()\n",
    "    model_dict = {}\n",
    "    for word in training.words:\n",
    "        model = model_selector(sequences, Xlengths, word, \n",
    "                        n_constant=3).select()\n",
    "        model_dict[word]=model\n",
    "    return model_dict\n",
    "\n",
    "# models = train_all_words(features_ground, SelectorConstant)\n",
    "# print(\"Number of word models returned = {}\".format(len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_selector=SelectorBIC, features=features_npodel\n",
      "\n",
      "**** WER = 0.5\n",
      "Total correct: 89 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "    2: *GO *BOOK HOMEWORK                                            JOHN WRITE HOMEWORK\n",
      "   67: *SHOULD FUTURE *MARY BUY HOUSE                                JOHN FUTURE NOT BUY HOUSE\n",
      "    7: JOHN CAN *JOHN CAN                                            JOHN CAN GO CAN\n",
      "  201: JOHN *MAN *WOMAN *LIKE BUY HOUSE                              JOHN TELL MARY IX-1P BUY HOUSE\n",
      "   74: *IX *GO VISIT *GO                                             JOHN NOT VISIT MARY\n",
      "  119: *MARY *BUY1 IX *JOHN *IX                                      SUE BUY IX CAR BLUE\n",
      "   12: JOHN *WHAT *GO1 *HOUSE                                        JOHN CAN GO CAN\n",
      "   77: *JOHN BLAME *LOVE                                             ANN BLAME MARY\n",
      "  142: JOHN BUY YESTERDAY WHAT BOOK                                  JOHN BUY YESTERDAY WHAT BOOK\n",
      "  107: *LIKE POSS *HAVE *GO *GO                                      JOHN POSS FRIEND HAVE CANDY\n",
      "   84: *LOVE *GIVE1 *GIVE1 BOOK                                      IX-1P FIND SOMETHING-ONE BOOK\n",
      "   21: JOHN *ARRIVE *FUTURE *WHO *CAR *TEACHER *MOTHER *FUTURE       JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: JOHN LIKE *LOVE *MARY IX                                      JOHN LIKE IX IX IX\n",
      "   89: JOHN *GIVE *IX *IX IX NEW *BOOK                               JOHN IX GIVE MAN IX NEW COAT\n",
      "   71: JOHN *FUTURE *GIVE1 MARY                                      JOHN WILL VISIT MARY\n",
      "   92: JOHN GIVE IX *IX WOMAN BOOK                                   JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   90: JOHN *HAVE IX SOMETHING-ONE WOMAN BOOK                        JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   30: JOHN LIKE *MARY *MARY IX                                      JOHN LIKE IX IX IX\n",
      "  193: JOHN *HAVE *YESTERDAY BOX                                     JOHN GIVE GIRL BOX\n",
      "   36: MARY VEGETABLE *YESTERDAY *GIVE *MARY *MARY                   MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "  139: JOHN *BUY1 WHAT *GIVE1 BOOK                                   JOHN BUY WHAT YESTERDAY BOOK\n",
      "  167: *MARY IX *VISIT LOVE MARY                                     JOHN IX SAY LOVE MARY\n",
      "   40: *BILL *GO *FUTURE1 MARY *MARY                                 JOHN IX THINK MARY LOVE\n",
      "   28: JOHN *MARY *FUTURE *MARY IX                                   JOHN LIKE IX IX IX\n",
      "  171: *MARY MARY BLAME                                              JOHN MARY BLAME\n",
      "   43: JOHN *JOHN BUY HOUSE                                          JOHN MUST BUY HOUSE\n",
      "  108: WOMAN *BOOK                                                   WOMAN ARRIVE\n",
      "  174: *JOHN *GIVE3 GIVE1 *GIRL *BLAME                               PEOPLE GROUP GIVE1 JANA TOY\n",
      "  113: IX CAR BLUE *IX *BUY1                                         IX CAR BLUE SUE BUY\n",
      "   50: *POSS *SEE BUY CAR *ARRIVE                                    FUTURE JOHN BUY CAR SHOULD\n",
      "  199: LIKE CHOCOLATE *TELL                                          LIKE CHOCOLATE WHO\n",
      "  158: LOVE JOHN *NOT                                                LOVE JOHN WHO\n",
      "   54: JOHN SHOULD *WHO BUY HOUSE                                    JOHN SHOULD NOT BUY HOUSE\n",
      "  105: JOHN *VEGETABLE                                               JOHN LEG\n",
      "  184: *IX BOY *GIVE1 TEACHER APPLE                                  ALL BOY GIVE TEACHER APPLE\n",
      "   57: *MARY *VISIT VISIT *IX                                        JOHN DECIDE VISIT MARY\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  189: *MARY *VISIT *VISIT BOX                                       JOHN GIVE GIRL BOX\n",
      "  181: *APPLE ARRIVE                                                 JOHN ARRIVE\n"
     ]
    }
   ],
   "source": [
    "from my_model_selectors import SelectorDIC, SelectorCV, SelectorBIC\n",
    "\n",
    "# selected features\n",
    "features_names = ['features_npodel','features_podel', 'features_ground']\n",
    "features_list = [features_npodel, features_podel, features_ground]\n",
    "# model_selectors\n",
    "model_selector_names = ['SelectorBIC', 'SelectorDIC', 'SelectorCV']\n",
    "model_selector_list = [SelectorBIC, SelectorDIC, SelectorCV]\n",
    "\n",
    "sel_features_list = features_list[1:2]\n",
    "sel_model_list = model_selector_list[2:3]\n",
    "# pre-build test_set\n",
    "test_set_list = [asl.build_test(features) for j, features in enumerate(sel_features_list)]\n",
    "\n",
    "for i, model_selector in enumerate(sel_model_list): \n",
    "    for j, features in enumerate(sel_features_list):\n",
    "        print(\"model_selector=%s, features=%s\" % (model_selector_names[i], features_names[j]))\n",
    "        models = train_all_words(features, model_selector_list[i])\n",
    "        test_set = test_set_list[j]\n",
    "        probabilities, guesses = recognize(models, test_set)\n",
    "        show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def more_guess(not_matched):\n",
    "    ret = {}\n",
    "    for wid, probs in not_matched.items():\n",
    "        h_scores = (probs[0][0][1], probs[1][0][1])  # highest in both probability list\n",
    "#         print(\"h_scores=\", h_scores)\n",
    "        dict_prob2 = dict(probs[1])\n",
    "        word, score = None, float(\"-inf\")\n",
    "        for ws_tuple in probs[0]:     # find the highest score\n",
    "            w = ws_tuple[0]     #\n",
    "            s = ws_tuple[1]\n",
    "            if ws_tuple[0] in dict_prob2:  # scores are neg\n",
    "                ws_1 = h_scores[0] - s \n",
    "                ws_2 = h_scores[1]-dict_prob2[w]\n",
    "                s = (ws_1 + ws_2) * -1 \n",
    "            if s > score:\n",
    "                score = s\n",
    "                word = w\n",
    "        ret[wid] = word\n",
    "#     print(\"more_guess\", ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_model_selectors import SelectorDIC, SelectorCV, SelectorBIC\n",
    "\n",
    "features_names = ['features_podel', 'features_polar', 'features_ground']   \n",
    "selector_names = [\"SelectorBIC\", 'SelectorDIC', 'SelectorCV']\n",
    "\n",
    "features_list = [features_podel, features_polar, features_ground] \n",
    "model_selector_list = [SelectorBIC, SelectorDIC, SelectorCV]   # change as needed\n",
    "test_set_list = [asl.build_test(features) for j, features in enumerate(features_list)]\n",
    "i = 0\n",
    "for j in range(len(features_list[:1])):\n",
    "    features = features_list[j]\n",
    "    print(\"model_selector_1=%s, model_selector_2=%s, feature=%s\" % (selector_names[i], selector_names[i+1], features_names[j]))\n",
    "    test_set = test_set_list[j]\n",
    "    models_1 = train_all_words(features, model_selector_list[i])\n",
    "    probabilities_1, guesses_1 = recognize(models_1, test_set)\n",
    "    models_2 = train_all_words(features, model_selector_list[i+1])\n",
    "    probabilities_2, guesses_2 = recognize(models_2, test_set)\n",
    "    \n",
    "    not_match = {}\n",
    "    num_not_match = 0\n",
    "    guesses = [None] * len(guesses_1)\n",
    "    guesses_dup = [None] * len(guesses_1)\n",
    "    for i, w1 in enumerate(guesses_1):\n",
    "        if w1 != guesses_2[i]:\n",
    "            num_not_match += 1\n",
    "            top_1 = sorted(probabilities_1[i].items(),key=lambda x: x[1], reverse=True)[0]\n",
    "            top_2 = sorted(probabilities_2[i].items(),key=lambda x: x[1], reverse=True)[0]\n",
    "            guesses[i] = top_1[0] if top_1[1] >= top_2[1] else top_2[0]\n",
    "\n",
    "            top3_1 = sorted(probabilities_1[i].items(),key=lambda x: x[1], reverse=True)[:3]\n",
    "            top3_2 = sorted(probabilities_2[i].items(),key=lambda x: x[1], reverse=True)[:3]\n",
    "            not_match[i] = (top3_1, top3_2)\n",
    "            \n",
    "        else:\n",
    "            guesses[i] = w1\n",
    "            guesses_dup[i] = w1\n",
    "    print(\"#total=%d, #not_match=%d\" % (len(guesses), num_not_match))\n",
    "    if num_not_match:\n",
    "        guesses_dup = guesses.copy()\n",
    "    show_errors(guesses, test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_errors(guesses_1, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autoreload for automatically reloading changes made in my_model_selectors and my_recognizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from my_model_selectors import SelectorConstant\n",
    "\n",
    "def train_all_words(features, model_selector):\n",
    "    training = asl.build_training(features)  # Experiment here with different feature sets defined in part 1\n",
    "    sequences = training.get_all_sequences()\n",
    "    Xlengths = training.get_all_Xlengths()\n",
    "    model_dict = {}\n",
    "    for word in training.words:\n",
    "        model = model_selector(sequences, Xlengths, word, \n",
    "                        n_constant=3).select()\n",
    "        model_dict[word]=model\n",
    "    return model_dict\n",
    "\n",
    "models = train_all_words(features_ground, SelectorConstant)\n",
    "print(\"Number of word models returned = {}\".format(len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_errors(guesses_2, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_errors(guesses_dup, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_model_selectors import SelectorDIC, SelectorCV, SelectorBIC, SelectorBIC_2\n",
    "\n",
    "# TODO Choose a feature set and model selector\n",
    "# TODO Recognize the test set and display the result with the show_errors method\n",
    "features_names = ['features_podel', 'features_polar', 'features_ground']   \n",
    "features_list = [features_podel, features_polar, features_ground] \n",
    "selector_names = [\"SelectorBIC\", 'SelectorDIC', \"SelectorBIC_2\", 'SelectorCV']\n",
    "model_selector_list = [SelectorBIC, SelectorDIC, SelectorBIC_2, SelectorCV]   # change as needed\n",
    "\n",
    "for i, model_selector in enumerate(model_selector_list[:1]):\n",
    "    for j, features in enumerate(features_list[:3]):\n",
    "        print(\"    $$$$$$ model_selector=%s, feature=%s\" % (selector_names[i], features_names[j]))\n",
    "        models = train_all_words(features, model_selector)\n",
    "        test_set = asl.build_test(features)\n",
    "        probabilities, guesses = recognize(models, test_set)\n",
    "        showtopwords(probabilities, i)\n",
    "        show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_means = asl.df.groupby('speaker').mean()\n",
    "df_std = asl.df.groupby('speaker').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_norm_pol = ['norm-rr', 'norm-rtheta', 'norm-lr','norm-ltheta'] \n",
    "#asl.df['norm-rx'] = asl.df['right-x']/asl.df['speaker'].map(df_std['right-x'])-(df_means['right-x']/df_std['right-x'])\n",
    "asl.df['norm-rr'] = (asl.df['polar-rr'] - asl.df['speaker'].map(df_means['polar-rr'])) \\\n",
    "    /asl.df['speaker'].map(df_std['polar-rr'])\n",
    "asl.df['norm-rtheta'] = (asl.df['polar-rtheta'] - asl.df['speaker'].map(df_means['polar-rtheta'])) \\\n",
    "    /asl.df['speaker'].map(df_std['polar-rtheta'])\n",
    "asl.df['norm-lr'] = (asl.df['polar-lr'] - asl.df['speaker'].map(df_means['polar-lr'])) \\\n",
    "    /asl.df['speaker'].map(df_std['polar-lr'])\n",
    "asl.df['norm-ltheta'] = (asl.df['polar-ltheta'] - asl.df['speaker'].map(df_means['polar-ltheta'])) \\\n",
    "    /asl.df['speaker'].map(df_std['polar-ltheta'])\n",
    "[asl.df.ix[98,1][v] for v in features_norm_pol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_norm_poldel = ['normpd-rr', 'normpd-rtheta', 'normpd-lr','normpd-ltheta']\n",
    "asl.df['normpd-rr'] = asl.df['norm-rr'] + asl.df['norm-rr'].diff().fillna(0)\n",
    "asl.df['normpd-rtheta'] = asl.df['norm-rtheta'] + asl.df['norm-rtheta'].diff().fillna(0)\n",
    "asl.df['normpd-lr'] = asl.df['norm-lr'] + asl.df['norm-lr'].diff().fillna(0)\n",
    "asl.df['normpd-ltheta'] = asl.df['norm-ltheta'] + asl.df['norm-ltheta'].diff().fillna(0)\n",
    "[asl.df.ix[98,36][v] for v in features_norm_poldel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def more_guess(not_matched):\n",
    "    ret = {}\n",
    "    for wid, probs in not_matched.items():\n",
    "        ret[wid] = probs[0][0][0] if probs[0][0][1] >= probs[1][0][1] else probs[1][0][0]\n",
    "    print(\"more_guess\", ret)\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
